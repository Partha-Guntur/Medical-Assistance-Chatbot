{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqydSD7KgaCF"
      },
      "source": [
        "1. Data Cleaning: Removing any NULL rows or columns.\n",
        "2. Data Preparation: Prepare the training data for the chatbot.\n",
        "3. BERT Embeddings: Use BERT to generate embeddings for the input text.\n",
        "4. LSTM Model: Build and train an LSTM model for generating responses.\n",
        "5. Chatbot Integration: Combine BERT and LSTM for the chatbot functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pS6V9nbHP_iV"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# For this demo, let's choose the first 1000 dialogues\n",
        "df = pd.DataFrame(train_data)\n",
        "df = df[[\"Description\", \"Doctor\"]].rename(columns={\"Description\": \"question\", \"Doctor\": \"answer\"})\n",
        "\n",
        "# Clean the question and answer columns\n",
        "df['question'] = df['question'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "question    0\n",
              "answer      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "question    0.0\n",
              "answer      0.0\n",
              "dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[df['question']==df['answer']].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat(message, history):\n",
        "  response = rag_chain.invoke(message)\n",
        "  return response\n",
        "\n",
        "# Create a Gradio interface\n",
        "with gr.Blocks() as interface:\n",
        "  # Display a welcome message (implementation omitted for brevity)\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      text_prompt = gr.Textbox(label=\"Input Prompt\", placeholder=\"Example: What are the symptoms of COVID-19?\", lines=2)\n",
        "      generate_button = gr.Button(\"Ask Me\", variant=\"primary\")\n",
        "  with gr.Row():\n",
        "    answer_output = gr.Textbox(type=\"text\", label=\"Answer\")\n",
        "  generate_button.click(chat, inputs=[text_prompt], outputs=answer_output)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch(server_name=\"0.0.0.0\", server_port=7000, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# Preprocess the dataset\n",
        "df = pd.DataFrame(train_data)\n",
        "df = df[[\"Description\", \"Doctor\"]].rename(columns={\"Description\": \"title\", \"Doctor\": \"text\"})\n",
        "df['title'] = df['title'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
        "\n",
        "# Convert DataFrame to Hugging Face Dataset\n",
        "data = Dataset.from_pandas(df)\n",
        "data = data.map(lambda example: {\"embeddings\": [0.0] * 768})  # Placeholder embeddings\n",
        "\n",
        "# Prepare the tokenizer and retriever\n",
        "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
        "retriever = RagRetriever.from_pretrained(\n",
        "    \"facebook/rag-sequence-nq\",\n",
        "    indexed_dataset=data\n",
        ")\n",
        "\n",
        "# Load the model\n",
        "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\")\n",
        "\n",
        "# Function for answering questions\n",
        "def answer_question(question):\n",
        "    input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True)[\"input_ids\"]\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids, \n",
        "        num_beams=5, \n",
        "        min_length=10, \n",
        "        max_length=50, \n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Gradio interface\n",
        "def chatbot_interface(question):\n",
        "    return answer_question(question)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Question Answering Chatbot\",\n",
        "    description=\"Ask a question and get an answer based on the dataset.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
